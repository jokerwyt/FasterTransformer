[structure]
t5_with_bias=true
position_embedding_type=relative
use_gated_activation=false
t5_with_moe=1

moe_layers_in_encoder=[1,3,5,7,9,11]
moe_layers_in_decoder=[1,3,5,7,9,11]

[encoder]
weight_data_type=fp32
d_model=768
vocab_size=32128
num_heads=12
d_kv=64
d_ff=3072
num_layers=12
relative_attention_num_buckets_or_max_pos_seq_len=32
num_experts=8

[decoder]
d_model=768
vocab_size=32128
num_heads=12
d_kv=64
d_ff=3072
num_layers=12
relative_attention_num_buckets_or_max_pos_seq_len=32
num_experts=8
decoder_start_token_id=0
eos_token_id=1
